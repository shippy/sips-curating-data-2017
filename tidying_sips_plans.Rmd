---
title: "Data curation: Tidy data & metadata"
author: Simon Podhajsky & David Condon
date: July 30, 2017
output: 
  revealjs::revealjs_presentation:
    theme: dark
    highlight: zenburn
---

## Outline

The steps needed to get a shareable dataset!

1. Getting your data set tidied up
2. Creating metadata documentation

# Part 1: Tidy data

## Slogan
> Write code for humans. Write data for computers.
>
> -- Vince Buffalo, [Bioinformatics Data Skills](http://shop.oreilly.com/product/0636920030157.do)

## First: What challenges do _you_ face?

## Note on file organization

```
 data/raw/
 data/clean/
 processing/
 analysis/
 README
```

We will be sharing two files _at minimum_

## Note on file formats

Preferred:

* plain text
* open (non-proprietary) format
    * I don't need to buy software to view this
    * Ideally, I don't need to _install_ software to view this

## Data is tidy if...

1. each observation occupies a single row
2. each variable occupies a single column
3. one type of observations resides in a single table.

## Data is tidy if...

1. each observation occupies a single row
2. each variable occupies a single column
3. one type of observations resides in a single table.

![](img/tidy-data.png)

<small>(source: [Chapter 12 of _R for Data Science_](http://r4ds.had.co.nz/tidy-data.html#fig:tidy-structure))</small>

## Easy?

## Yeah, right

> * Is observation...
>     * a survey submission? 
>     * each response in that submission?
> * What separates kinds of observation? 
> * What separates observations from variables? (`work_phone` vs. `home_phone`)
> * Might depend on context and discretion, but broad intuitions apply broadly

## Negative take: tidy ~ not messy

Messy data is usually messy in the following ways:

> * Column headers are values, not variable names
> * Multiple variables are stored in one column
> * Variables are stored in both rows and columns
> * Multiple types of experimental unit stored in the same table
> * One type of experimental unit stored in multiple tables

We'll go over each in the example.

## Origin of "tidy data"

[Hadley Wickham's (2014) Tidy Data paper.](https://www.jstatsoft.org/article/view/v059i10) Do read it if you can.

(much of this presentation is modeled on [his original slides](http://stat405.had.co.nz/lectures/18-tidy-data.pdf))

# Messy dataset example: Your plans for SIPS

```{r, eval=FALSE, include=FALSE}
# This is how I edited the original export
library(tidyverse)
library(generator)
x <- read_csv('data/raw/plans_original.csv')
x$Name <- r_full_names(nrow(x))
x$`Email address` <- r_email_addresses(nrow(x))
x <- x[, 1:6]
head(x)
glimpse(x)
write_csv(x, 'data/raw/plans_raw.csv')
```

----

* Remember [that form you filled out?](SIPS 2017 RSVP Survey.pdf) We'll look at the CSV output from Google Forms.
* (Examples are a little `R` heavy, but many tools do the same)
* Only code excerpts shown; for full code, [check out the RMarkdown source of this presentation.](https://github.com/shippy/sips-curating-data-2017/blob/master/tidying_sips_plans.Rmd)

```{r, include=FALSE}
library(tidyverse)
library(knitr)
x <- read_csv('data/raw/plans_raw.csv')
```

## Dataset excerpt (1/2)

```{r, asis=TRUE}
knitr::kable(x[106, 1:4], row.names = FALSE)
```

## Dataset excerpt (2/2)

```{r, echo=FALSE, asis = TRUE}
knitr::kable(x[106, 5:6], row.names = FALSE)
```

## What are the problems here?
> * Unwieldy column names
> * Ambiguous commas - some separate plans, others are used in a sentence
>     * "Workshop: Using OSF (1:30-3:30p), Workshop: Writing papers to be transparent, reproducible, and fabulous (3:30-5:30p)"
> * Identifying personal information
> * Data mistakes?
>     * Brian's social event is listed on the wrong day
>     * What timezone is the datetime? _(More in the *Metadata* section)_
>     * Double submissions?
> * And, of course, not tidy

----

```{r}
# Read the file in with better headers
library(readr)
x <- read_csv('data/raw/plans_raw.csv',
              col_names = c("time_submitted", "name", "email", 
                            "day1", "day2", "day3"),
              col_types = list(
                col_datetime(format = '%m/%d/%Y %H:%M:%S'), 
                col_character(), col_character(), 
                col_character(), col_character(),
                col_character()),
              skip = 1) # skip row with headers
head(x)
```

----

```{r}
knitr::kable(x[106, 4:6], row.names = FALSE)
```

Now, let's talk about what makes this messy.

## Mess #1: mixing different data sets
> * Our datasets centers on plans for specific events. Why should we have people's emails here?
> * If we need to do any processing on a per-person level, things become unwieldy fast.

![](img/tidy-data.png)

----

**Tidy solution:** divide the data units into tables you can merge as needed

```{r}
people <- select(x, time_submitted, name, email) %>% unique()
people$data_origin <- 'plans_raw.csv' # if many possible sources

# if non-existent, create an ID to join the tables by. 
# (Usually, you might have an ID for each participant at 
# this point, which you'd now deidentify.)
people$participant_ID = sample(1:nrow(people), nrow(people))

# remove the people-specific data from `x`
x <- merge(x, people) %>% select(-time_submitted, -name, 
                                 -email, -data_origin)
```

> * _**Question:** Should we separate the table of events and plans, too?_

----

As a bonus, sensitive info is relegated to a single spot, and is easier to deal with - for example, extract non-identifying features of interest:

```{r}
# dplyr::transmute drops all columns that are not explicitly named
people <- transmute(people, 
                    time_submitted, participant_ID,
                    email_server = gsub('.+@([^@]+)$', '\\1', email),
                    number_names = length(strsplit(name, ' ')[[1]])
)
```
```{r, echo=FALSE, asis = TRUE}
knitr::kable(people[1, ], row.names = FALSE)
```

Of course, this might still be identifying information - consult your local HIPAA board.

## Mess #2: Data stored in column names

> * Each column stores the date on which the event is held.
> * Even though we renamed it from `What are you planning to do on August 1` to `day1`, that is still plan-level info.


```{r, echo = FALSE}
knitr::kable(x[106, ], row.names = FALSE)
```

----

**Tidy solution:** translate the data from wide to long

```{r}
x <- tidyr::gather(x, event_day, event, -participant_ID)
x$event_day <- as.numeric(gsub('day', '', x$event_day)) # clean up
```
```{r, echo = FALSE}
knitr::kable(x[106:107, ], row.names = FALSE)
```

* _Side effect to note for later: some people are now planning for `NA`. Why?_

## Mess #3: Multiple records in a single row
> * As many plans as there were checkboxes
> * Doing any analysis of event attendance is hard

----

**Tidy solution:** One row per one intention to attend an event

```{r, echo = FALSE}
# we want to quote the content of each cell, or escape the commas that aren't descriptive
# create ad-hoc function to do this correct when needed
escape_multiple_values <- function(values) {
  library(stringr)
  # got these unique strings from Ctrl+F the form, but in larger datasets, things might be more difficult
  to_quote = c("Saturday, July 29th, 7p", "Part 1: IRB, Part 2: Curating data", 
               "transparent, reproducible, and fabulous", 
               "department, university, society", 
               "if there is a hackathon that needs help, let me know")
  # we temporarily escape the separators
  replacements = str_replace_all(to_quote, ',', '@+@')
  names(replacements) <- to_quote

  str_replace_all(values, replacements)
}
 
# escape_multiple_values(c("[Saturday, July 29th, 7p] Dinner/drinks party at Brian Nosek's home (bus transportation to/from Omni hotel all evening), Workshop: Fundamentals of meta-analysis (9:30a-12:30p), Workshop: How to Promote Transparency and Replicability as a Reviewer (1:30-3:30p), Workshop: Writing papers to be transparent, reproducible, and fabulous (3:30-5:30p)", "Workshop: How to Promote Transparency and Replicability as a Reviewer (1:30-3:30p), Workshop: Writing papers to be transparent, reproducible, and fabulous (3:30-5:30p), Hack: Resources for changing culture in department, university, society (all day), I am flexible; if there is a hackathon that needs help, let me know."))

unescape_multiple_values <- function(values) {
  str_replace_all(values, '@\\+@', ',')
}
```

```{r}
# not shown: function that escapes non-separator commas
x$event <- escape_multiple_values(x$event)
x <- separate_rows(x, event, sep = ', ')
# unescape the event titles
x$event <- unescape_multiple_values(x$event)
```
----

We can now easily fix the mis-dating of Brian's social event, which - as an artifact of survey design - would otherwise still be considered a Day 1 event:

```{r}
# With just one mis-dated event, we can just re-date it
x$event_day[x$event == paste0("[Saturday, July 29th, 7p] ", 
  "Dinner/drinks party at Brian Nosek's home (bus ",
  "transportation to/from Omni hotel all evening)")] <- 0
```

## Mess #4: Multiple variables in a single column
```{r, echo = FALSE}
knitr::kable(x[106, ], row.names = FALSE)
```

What's wrong here?

> * All mashed up in `event`: event time, event name, **and** event type

----

```{r, echo = FALSE, include = FALSE}
# Mostly constant position (Type: name (time)), so extract it like that
# This won't work for all
xtest <- extract(x, event, 
                 c('event_type', 'event_name', 'event_time'), 
                 regex = "(^[[:alnum:]-]+): (.+) \\((.+)\\)$", 
                 remove = FALSE)

# What events did this not work for?
xtest %>% filter(is.na(event_type)) %>% select(event) %>% unique() 
```

```{r, echo = FALSE}
# modify events to fit in the regex
x$event[x$event == "[Saturday, July 29th, 7p] Dinner/drinks party at Brian Nosek's home (bus transportation to/from Omni hotel all evening)"] <- "Social: Dinner/drinks party at Brian Nosek's home (7p-?)"
x$event[x$event == "All Conference Social Event (Dinner+drinks; 7:30p-??)"] <- "Social: All Conference Dinner & Drinks (7:30p-??)"
```

```{r}
# Structure of `event` is stable (Type: name (time)), so 
# a regular expression can extract it
#
# (Not shown: slight data manipulation to make this work 
#  for all events)
x <- extract(x, event, 
             c('event_type', 'event_name', 'event_time'), 
             regex = "(^[[:alnum:]-]+): (.+) \\((.+)\\)$", 
             remove = FALSE)
```
```{r, echo = FALSE}
knitr::kable(x[106, ], row.names = FALSE)
```

## Mess #5: Multiple types of experimental unit in the same table

> * "I'm flexible" isn't a plan for a specific event.
> * _**Question**: Should such responses be removed or re-defined? When should curation remove responses?_
>     * _Relevant: What does a plan for `NA` event mean?_

```{r, echo = FALSE}
knitr::kable(x[114, ], row.names = FALSE)
```

----

```{r}
flexible_responses <- c("Everything else will be scheduled during the conference depending on what has legs and group interest", 
                        "I am flexible; if there is a hackathon that needs help, let me know.")

# Removal:
# x <- filter(x, !(event %in% flexible_responses))

# Re-definition
x$event_type[x$event %in% flexible_responses] <- "Flexible"
```

```{r, echo = FALSE}
knitr::kable(x[114, ], row.names = FALSE)
```

## Mess #6: One type of experimental unit stored in multiple tables

> * Bullet we dodged, but can you imagine examples?
> * If the Plans GForm asked for participant dietary restrictions, and the Lightning Talk GForm asked for participant age, both should be extracted into logical places

## Could we make things even cleaner?
> * Add `event_date`
> * Convert `event_time` to `event_starttime` and `event_duration`
> * Remove `event`, as we've extracted everything out of it
> * In fact, separate out `events.csv` into its own table and join by `event_id`

## Real artists ~~ship~~ publish

```{r}
x <- select(x, -event)
write_csv(x, 'data/clean/event_plans.csv')
write_csv(people, 'data/clean/people.csv')
save(people, x, file = 'data/clean/all_data.Rdata')
```
```{r, echo = FALSE}
knitr::kable(x[106, ], row.names = FALSE)
```


# Things are easier with a tidy data set

## Tidy data are like a toothpaste

> * A tidy dataset is easy to push into "messy" forms, but backwards doesn't work quite as well.

## Quick exploration is easy

```{r, eval=FALSE, include=FALSE}
library(ggplot2)
library(forcats)
theme_set(theme_bw() + theme(title = element_text(size = 16), 
                             axis.title = element_text(size = 13),
                             axis.text = element_text(size = 12)))

popularity <- ggplot(x %>% filter(event_day %in% 1:2), 
                     aes(x = fct_infreq(event_name), fill = event_type)) + 
  geom_bar() + 
  xlab("") + ylab("# of interested attendees") +
  ggtitle("Which events are most appealing?") +
  theme_bw() + theme(axis.text.x = element_text(angle = 70, hjust = 1))
popularity
popularity + 
  facet_wrap(~ event_day, scales = "free", nrow = 1, 
             labeller = as_labeller(function(x) paste0("Day ", x))) +
  ggtitle("Which events are most appealing in each day?")
```

```{r, echo = FALSE}
ggplot(people,
       aes(x = time_submitted)) + 
  stat_bin(aes(y = cumsum(..count..)), geom="step", binwidth = 1800) +
  ggtitle("How fast do SIPS attendees respond to organizer requests?") + 
  xlab("Date") + ylab("Cumulative number of responses") +
  scale_x_datetime(date_breaks = "1 days") +
  theme_bw() + theme(axis.text.x = element_text(angle = 30, hjust = 1))
```


----

```{r, echo = FALSE}
all = merge(x, people) # by = 'participant_ID' is implied
ggplot(all %>% group_by(participant_ID, time_submitted) %>% summarize(number_plans = n()), 
       aes(y = number_plans, x = time_submitted)) + 
  geom_jitter(height = 0.1, alpha = 0.9) +
  ggtitle("Do late submitters plan to attend fewer events?") + 
  xlab("Submission date") + ylab("Number of planned events") +
  theme_bw() + 
  scale_x_datetime(date_breaks = "1 days") + theme(axis.text.x = element_text(angle = 30, hjust = 1))
```

# Tidy tools

## It's not just R

* [OpenRefine](https://openrefine.org) (formerly Google Refine)
* [Trifacta Wrangler](https://www.trifacta.com/products/wrangler/) (formerly [Data Wrangler](http://vis.stanford.edu/wrangler/))
* Excel "Table" + pivot table shenanigans?
* All statistical software has some version of reshaping tools

----

...but if you're looking for R, [`tidyverse`](http://tidyverse.org/) is excellent

# Your tidy woes?

# Part 2: Metadata

----

* How the data was collected & why --- study- and table-level documentation
* Follows naturally from a clean dataset
* Allows for usability and findability

## Always be documenting

![](img/metadata-circle.png)

<small>(source: [Wendy Thomas and Marcel Hebing 2013 presentation on DDI](http://www.ddialliance.org/system/files/Thomas-Hebing-2013-EDDI.pptx))</small>

# Option 1: Standardized metadata

## Standardized metadata

* There are many standards~~, and they have a plan~~
* Most common in social science: [DDI (Documenting Data Initiative)](http://www.ddialliance.org/)
    * Lifecycle standard
    * Slimmed-down Codebook standard
    * [Examples on the DDI website](http://www.ddialliance.org/resources/markup-examples)
* Alternative standards exist: [Dublin Core](http://dublincore.org/), [MINSEQE](http://fged.org/projects/minseqe/) (genomics), [MIBBI](https://biosharing.org/standards/?selected_facets=isMIBBI:true&view=table) (biology + biomedicine), ...

----

![](img/xkcd_standards.png)

<small>(source: <https://xkcd.com/927/>)</small>

## More on DDI

## Pros

> * Very, **very** thoroughly standardized & documented
> * XML means machine-readability
> * Connects into other existing frameworks
> * Ideal for extensive datasets

## Cons

* Far too thoroughly standardized - steep learning curve
* XML does not mean human readability
* Overkill for small-ish datasets

## Tools

* [Colectica software](http://www.colectica.com), especially the free [Colectica for Excel](http://www.colectica.com/software/colecticaforexcel) (Windows-only)
* [Archivist](https://github.com/CLOSER-Cohorts/archivist) by UK's CLOSER
* [Nesstar](http://www.nesstar.com/software/publisher.html)
* [...and DDI has a comprehensive list of others](https://www.ddialliance.org/resources/tools)

# Option 2: "Readme" metadata

## "Readme" metadata

* Flexible description in plain text
* Many templates available - [Cornell Library has an excellent one](https://cornell.app.box.com/v/ReadmeTemplate)

# Documenting our SIPS dataset

## We'll be using the Cornell template

## Basics
1. Title of Dataset: **Preliminary plans of SIPS 2017 attendeees**
2. Author Information
    * Principal Investigator Contact Information
          * Name: **David Condon**
              * Institution: Northwestern University
              * Email: david-condon@northwestern.edu
    * Associate or Co-investigator Contact Information
          * Name: **Simon Podhajsky**
              * Institution: Yale University
              * Email: simon.podhajsky@yale.edu
3. Date of data collection (single date, range, approximate date) **20170713-20170725**

## Basics (cont'd)
4. Geographic location of data collection (where was data collected?): **online**
5. Information about funding sources that supported the collection of the data: **COS/SIPS and its funders**

## Sharing / access information
1. Licenses/restrictions placed on the data: **MIT**
2. Links to publications that cite or use the data: -
3. Links to other publicly accessible locations of the data: -
4. Links/relationships to ancillary data sets: **~~SIPS 2017 registrations~~**
5. Was data derived from another source? -
6. Recommended citation for the data: **_We'll soon have one!_**

## Data and file overview
1. File List
    a. Filename: **`people.csv`** 
        * Short description: **Properties of SIPS attendees**
    b. Filename: **`event_plans.csv`**     
        * Short description: **Plans for events by participant, with description of events**
2. Relationship between files: **`people.csv` provides participant data for `event_plans.csv` (one-to-many)**
3. Additional related data collected that was not included in the current data package: **~~flexible plans,~~ participant registration**

## Data and file overview (cont'd)
4. Are there multiple versions of the dataset? **no**
    * If yes, list versions: -
    * Name of file that was updated: -
        i. Why was the file updated? -
        ii. When was the file updated? -
        
_Question: is this necessary if you use version control?_

## Methodological information
1. Description of methods used for collection/generation of data (include references): **Google Form sent to registered participants via e-mail**
2. Methods for processing the data: **Export to CSV, then run `tidying_sips_plans.Rmd`**
3. Instrument- or software-specific information needed to interpret the data: **none, but R 3.4+ preferred**
4. Standards and calibration information, if appropriate: **-**

## Methodological information (cont'd)
5. Environmental/experimental conditions: **-**
6. Describe any quality-assurance procedures performed on the data: **-**
7. People involved with sample collection, processing, analysis and/or submission: **Katie & Brian & ... (collection), Simon (processing & analysis), David (submission)**

## `event_plans.csv`

1. Number of variables: **5**
2. Number of cases/rows: **1260**
3. Variable List
     a. Name: **`participant_ID`**
          * Description: **unique participant index linked to the person data in `people.csv`**
     b. Name: **`event_type`**
          * Description: **The format of the event the participant is planning to attend**
          * Possible values: **Social, Hack, Re-hack, Workshop, Flexible, `NA`** _(and meanings)_
     c. Name: **...**

## `event_plans.csv` (cont'd)

4. Missing data codes: **-**
5. Specialized formats of other abbreviations used: **-**

## `people.csv`
...

# Resources

----

* Tidyverse ([original paper](https://www.jstatsoft.org/article/view/v059i10), presentation, write-up, & book)
* [R data wrangling cheatsheet](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf)
* [Cornell's Research Data Management Service Group on metadata](https://data.research.cornell.edu/content/writing-metadata) (and other articles)
* UC Mercedes recommendations on [data curation](http://library.ucmerced.edu/research/researchers/research-data-curation), [file management](http://library.ucmerced.edu/research/researchers/research-data-management/best-practices-managing-your-data), and [archiving + sharing](http://library.ucmerced.edu/research/researchers/research-data-management/archiving-and-sharing-your-data)
* [Documenting Data Initiative](http://www.ddialliance.org)
    * The [presentations in the Training Library](http://www.ddialliance.org/training/training-library) are easiest to digest
* University of Michigan's [ICPSR](https://www.icpsr.umich.edu), especially its [Guide to Social Science Data Preparation and Archiving](https://www.icpsr.umich.edu/icpsrweb/content/deposit/guide/index.html)